<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Alan  Ramponi


  | publications

</title>
<meta name="description" content="Alan Ramponi website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêØ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Alan</span>   Ramponi
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                short cv
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <p>A list of peer-reviewed publications and associated materials is presented below. You can also check my <a href="https://scholar.google.com/citations?user=Yxd4YqYAAAAJ" target="\_blank">Google Scholar</a> page. Note that NLP, and AI more broadly, are conference-driven fields. For top-tier venues on the subject, see <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computationallinguistics" target="\_blank">this page</a>.</p>

<div class="publications">

<h2>Conference papers</h2>

  <ol class="bibliography"><li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="ramponi-tonelli-2022-features" class="col-sm-10">
    <div class="title">Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Tonelli, S.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>,
    
    
      2022
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective data-centric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data stataments practice.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>

  <ol class="bibliography">
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="van-der-goot-etal-2021-masked" class="col-sm-10">
    <div class="title">From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Van Der Goot, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sharaf, I.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Imankulova, A.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  √úst√ºn, A.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stepanoviƒá, M.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khairunnisa, S. O.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Komachi, M.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Plank, B.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>,
    
    
      2021
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2021.naacl-main.197/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2021.naacl-main.197.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://bitbucket.org/robvanderg/xsid/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      <a href="https://www.youtube.com/watch?v=DH0C-n_p6h0" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    
      
      <a href="/assets/docs/2021.naacl-main.poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/docs/2021.naacl-main.slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      
      <a href="https://aclanthology.org/2021.naacl-main.197.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="van-der-goot-etal-2021-massive" class="col-sm-10">
    <div class="title">Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Van Der Goot, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  √úst√ºn, A.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sharaf, I.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Plank, B.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</em>,
    
    
      2021
    
    
      ‚Äî <span style="font-weight: bold; color: var(--global-theme-color);">
        Outstanding paper award <img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20">
      </span>
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2021.eacl-demos.22/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2021.eacl-demos.22.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/machamp-nlp/machamp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      <a href="https://www.youtube.com/watch?v=DauTEdMhUDI" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    
      
      <a href="/assets/docs/2021.eacl-demos.poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://machamp-nlp.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
      
      <a href="https://aclanthology.org/2021.eacl-demos.22.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>

  <ol class="bibliography">
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLING</abbr>
    
  
  </div>

  <div id="ramponi-plank-2020-neural" class="col-sm-10">
    <div class="title">Neural Unsupervised Domain Adaptation in NLP---A Survey</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Plank, B.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 28th International Conference on Computational Linguistics</em>,
    
    
      2020
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2020.coling-main.603/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2020.coling-main.603.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/docs/2020.coling-main.poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
      <a href="https://github.com/bplank/awesome-neural-adaptation-in-NLP" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
      
      <a href="https://aclanthology.org/2020.coling-main.603.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="ramponi-etal-2020-biomedical" class="col-sm-10">
    <div class="title">Biomedical Event Extraction as Sequence Labeling</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Van Der Goot, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lombardo, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Plank, B.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>,
    
    
      2020
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2020.emnlp-main.431/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2020.emnlp-main.431.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/cosbi-research/beesl" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      <a href="https://slideslive.com/38939154" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    
    
      
      <a href="/assets/docs/2020.emnlp-main.slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      
      <a href="https://aclanthology.org/2020.emnlp-main.431.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BeeSL's speed and accuracy makes it a viable approach for large-scale real-world scenarios.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">LREC</abbr>
    
  
  </div>

  <div id="ramponi-etal-2020-cross" class="col-sm-10">
    <div class="title">Cross-Domain Evaluation of Edge Detection for Biomedical Event Extraction</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Plank, B.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Lombardo, R.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 12th Language Resources and Evaluation Conference</em>,
    
    
      2020
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2020.lrec-1.244/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2020.lrec-1.244.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://www.cosbi.eu/cfx/9985" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    
      
      <a href="https://aclanthology.org/2020.lrec-1.244.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Biomedical event extraction is a crucial task in order to automatically extract information from the increasingly growing body of biomedical literature. Despite advances in the methods in recent years, most event extraction systems are still evaluated in-domain and on complete event structures only. This makes it hard to determine the performance of intermediate stages of the task, such as edge detection, across different corpora. Motivated by these limitations, we present the first cross-domain study of edge detection for biomedical event extraction. We analyze differences between five existing gold standard corpora, create a standardized benchmark corpus, and provide a strong baseline model for edge detection. Experiments show a large drop in performance when the baseline is applied on out-of-domain data, confirming the need for domain adaptation methods for the task. To encourage research efforts in this direction, we make both the data and the baseline available to the research community: https://www.cosbi.eu/cfx/9985.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">LREC</abbr>
    
  
  </div>

  <div id="van-der-goot-etal-2020-norm" class="col-sm-10">
    <div class="title">Norm It! Lexical Normalization for Italian and Its Downstream Effects for Dependency Parsing</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Van Der Goot, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Caselli, T.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cafagna, M.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and De Mattei, L.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the 12th Language Resources and Evaluation Conference</em>,
    
    
      2020
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2020.lrec-1.769/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2020.lrec-1.769.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://bitbucket.org/robvanderg/normit" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    
      
      <a href="https://aclanthology.org/2020.lrec-1.769.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Lexical normalization is the task of translating non-standard social media data to a standard form. Previous work has shown that this is beneficial for many downstream tasks in multiple languages. However, for Italian, there is no benchmark available for lexical normalization, despite the presence of many benchmarks for other tasks involving social media data. In this paper, we discuss the creation of a lexical normalization dataset for Italian. After two rounds of annotation, a Cohen's kappa score of 78.64 is obtained. During this process, we also analyze the inter-annotator agreement for this task, which is only rarely done on datasets for lexical normalization, and when it is reported, the analysis usually remains shallow. Furthermore, we utilize this dataset to train a lexical normalization model and show that it can be used to improve dependency parsing of social media data. All annotated data and the code to reproduce the results are available at: http://bitbucket.org/robvanderg/normit.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>


<h2>Workshop papers</h2>

  <ol class="bibliography"></ol>

  <ol class="bibliography"><li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">W-NUT@EMNLP</abbr>
    
  
  </div>

  <div id="van-der-goot-etal-2021-multilexnorm" class="col-sm-10">
    <div class="title">MultiLexNorm: A Shared Task on Multilingual Lexical Normalization</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Van Der Goot, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zubiaga, A.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Plank, B.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Muller, B.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  San Vicente Roncal, I.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ljube≈°iƒá, N.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  √áetinoƒülu, √ñ.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mahendra, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  √áolakoƒülu, T.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Baldwin, T.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Caselli, T.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sidorenko, W.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>In Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)</em>,
    
    
      2021
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://aclanthology.org/2021.wnut-1.55/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://aclanthology.org/2021.wnut-1.55.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://bitbucket.org/robvanderg/multilexnorm/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      
      <a href="/assets/docs/2021.wnut.slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
      <a href="http://noisy-text.github.io/2021/multi-lexnorm.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
      
      <a href="https://aclanthology.org/2021.wnut-1.55.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Lexical normalization is the task of transforming an utterance into its standardized form. This task is beneficial for downstream analysis, as it provides a way to harmonize (often spontaneous) linguistic variation. Such variation is typical for social media on which information is shared in a multitude of ways, including diverse languages and code-switching. Since the seminal work of Han and Baldwin (2011) a decade ago, lexical normalization has attracted attention in English and multiple other languages. However, there exists a lack of a common benchmark for comparison of systems across languages with a homogeneous data and evaluation setup. The MultiLexNorm shared task sets out to fill this gap. We provide the largest publicly available multilingual lexical normalization benchmark including 13 language variants. We propose a homogenized evaluation setup with both intrinsic and extrinsic evaluation. As extrinsic evaluation, we use dependency parsing and part-of-speech tagging with adapted evaluation metrics (a-LAS, a-UAS, and a-POS) to account for alignment discrepancies. The shared task hosted at W-NUT 2021 attracted 9 participants and 18 submissions. The results show that neural normalization systems outperform the previous state-of-the-art system by a large margin. Downstream parsing and part-of-speech tagging performance is positively affected but to varying degrees, with improvements of up to 1.72 a-LAS, 0.85 a-UAS, and 1.54 a-POS for the winning system.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>

  <ol class="bibliography"></ol>


<h2>Journal papers</h2>

  <ol class="bibliography"></ol>

  <ol class="bibliography"><li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Front. Cell Dev. Biol.</abbr>
    
  
  </div>

  <div id="parolo-2021-reconstruction" class="col-sm-10">
    <div class="title">Reconstruction of the Cytokine Signaling in Lysosomal Storage Diseases by Literature Mining and Network Analysis</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Parolo, S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tomasoni, D.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bora, P.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kaddi, C.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Azer, K.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Domenici, E.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Neves-Zaph, S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Lombardo, R.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>Frontiers in Cell and Developmental Biology</em>,
    
    
      2021
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://www.frontiersin.org/article/10.3389/fcell.2021.703489" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://www.frontiersin.org/articles/10.3389/fcell.2021.703489/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
      
      <a href="/assets/docs/2021.front-cell-dev-biol.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Lysosomal storage diseases (LSDs) are characterized by the abnormal accumulation of substrates in tissues due to the deficiency of lysosomal proteins. Among the numerous clinical manifestations, chronic inflammation has been consistently reported for several LSDs. However, the molecular mechanisms involved in the inflammatory response are still not completely understood. In this study, we performed text-mining and systems biology analyses to investigate the inflammatory signals in three LSDs characterized by sphingolipid accumulation: Gaucher disease, Acid Sphingomyelinase Deficiency (ASMD), and Fabry Disease. We first identified the cytokines linked to the LSDs, and then built on the extracted knowledge to investigate the inflammatory signals. We found numerous transcription factors that are putative regulators of cytokine expression in a cell-specific context, such as the signaling axes controlled by STAT2, JUN, and NR4A2 as candidate regulators of the monocyte Gaucher disease cytokine network. Overall, our results suggest the presence of a complex inflammatory signaling in LSDs involving many cellular and molecular players that could be further investigated as putative targets of anti-inflammatory therapies.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>

  <ol class="bibliography"><li>

<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE Access</abbr>
    
  
  </div>

  <div id="ramponi-etal-2020-high" class="col-sm-10">
    <div class="title">High-Precision Biomedical Relation Extraction for Reducing Human Curation Efforts in Industrial Applications</div>
    <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ramponi, A.</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Giampiccolo, S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tomasoni, D.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Priami, C.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Lombardo, R.
                
              
            
          
        
    </div>

    <div class="periodical">
    
      <em>IEEE Access</em>,
    
    
      2020
    
    
    
    </div>

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      <a href="https://ieeexplore.ieee.org/document/9171821" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://ieeexplore.ieee.org/document/9171821" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
      
      <a href="/assets/docs/2020.ieee-access.bib" class="btn btn-sm z-depth-0" role="button" target="_blank">Bib</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The body of biomedical literature is growing at an unprecedented rate, exceeding the ability of researchers to make effective use of this knowledge-rich amount of information. This growth has created interest in biomedical relation extraction approaches to extract domain-specific knowledge for diverse applications. Despite the great progress in the techniques, the retrieved evidence still needs to undergo a time-consuming manual curation process to be truly useful. Most relation extraction systems have been conceived in the context of Shared Tasks, with the goal of maximizing the F1 score on restricted, domain-specific test sets. However, in industrial applications relations typically serve as input to a pipeline of biologically driven analyses; as a result, highly precise extractions are central for cutting down the manual curation effort, thus to translate the research evidence into practice smoothly and reliably. In this paper, we present a highly precise relation extraction system designed to reduce human curation efforts. The engine is made up of sophisticated rules that leverage linguistic aspects of the texts rather than sticking on application-specific training data. As a result, the system could be applied to diverse needs. Experiments on gold-standard corpora show that the system achieves the highest precision compared with previous rule-based, kernel-based, and neural approaches, while maintaining a F1 score comparable or superior to other methods. To show the usefulness of our approach in industrial scenarios, we finally present a case study on the mTOR pathway, showing how it could be applied on a large-scale.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>


<h2>Other publications</h2>
<ol class="bibliography"><li>
  <div class="title">
    <a href="http://hdl.handle.net/11572/310787" target="_blank">Knowledge Extraction from Biomedical Literature with Symbolic and Deep Transfer Learning Methods</a>
  </div>
  <div class="author">
    
      
      
      
      
          
      
        
          <em>Ramponi, A.</em>,
        
      
    
    
      PhD thesis.
    
    
      University of Trento, Italy,
    
    
      2021
    
  </div>

</li></ol>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    ¬© Copyright 2022 Alan  Ramponi.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
